{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Al9Zk0jTtbwi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd  # 用于数据处理\n",
        "import numpy as np  # 用于矩阵计算\n",
        "import torch  # 用于搭建及训练模型\n",
        "import time  # 用于训练计时\n",
        "import random  # 用于生成随机数\n",
        "import os  # 用于文件操作\n",
        "from torchtext import data  # 用于生成数据集\n",
        "from torchtext.vocab import Vectors  # 用于载入预训练词向量\n",
        "from tqdm import tqdm  # 用于绘制进度条\n",
        "from torchtext.data import Iterator, BucketIterator  # 用于生成训练和测试所用的迭代器\n",
        "import torch.nn as nn  # 用于搭建模型\n",
        "import torch.optim as optim  # 用于生成优化函数\n",
        "from matplotlib import pyplot as plt  # 用于绘制误差函数\n",
        "#from gensim.models import Word2Vec\n",
        "import os.path as path\n",
        "import codecs\n",
        "\n",
        "torch.manual_seed(19260817)  # 设定随机数种子\n",
        "torch.backends.cudnn.deterministic = True  # 保证可复现性\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def tokenize(x): return x.split()  # 分词函数,后续操作中会用到"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFGV1oAotlMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wordvecPath = \"word2vec.vector\"\n",
        "dataPath = \"/content/drive/My Drive/Colab Notebooks/dataset/\"\n",
        "#wordVec = Word2Vec.load(wordvecPath)\n",
        "TEXT = data.Field(sequential=True, tokenize=tokenize)\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "    name = 'Dataset'\n",
        "    def __init__(self, fin, text_field):\n",
        "        fields = [(\"text\", text_field)]\n",
        "        examples = []\n",
        "        print('read data from {}'.format(path))\n",
        "        for line in fin:\n",
        "            examples.append(data.Example.fromlist([line], fields))\n",
        "        super(Dataset, self).__init__(examples, fields) #生成标准dataset\n",
        "\n",
        "\n",
        "def getDataIter(fin, fiveOrSeven):\n",
        "    data = Dataset(fin, TEXT)\n",
        "    vectors = Vectors(name='/content/drive/My Drive/Colab Notebooks/word2vec.vector')\n",
        "    TEXT.build_vocab(data, vectors=vectors, unk_init = torch.Tensor.normal_, min_freq=5) #构建映射,设定最低词频为5\n",
        "    return BucketIterator(dataset=data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "def getTrainIter(fiveOrSeven):\n",
        "    assert fiveOrSeven == 5 or fiveOrSeven == 7\n",
        "    trainfin = codecs.open(path.join(dataPath, \"qtrain\"+str(fiveOrSeven)), 'r', encoding = 'utf-8')\n",
        "    return getDataIter(trainfin, fiveOrSeven)\n",
        "\n",
        "def getTestIter(fiveOrSeven):\n",
        "    assert fiveOrSeven == 5 or fiveOrSeven == 7\n",
        "    testfin = codecs.open(path.join(dataPath, \"qtest\"+str(fiveOrSeven)), 'r', encoding = 'utf-8')\n",
        "    return getDataIter(testfin, fiveOrSeven)\n",
        "\n",
        "def getValidIter(fiveOrSeven):\n",
        "    assert fiveOrSeven == 5 or fiveOrSeven == 7\n",
        "    validfin = codecs.open(path.join(dataPath, \"qvalid\"+str(fiveOrSeven)), 'r', encoding = 'utf-8')\n",
        "    return getDataIter(validfin, fiveOrSeven)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJX1vd2euuaH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "37905347-be3f-49ad-bae9-b808c6bb1985"
      },
      "source": [
        "batch_size = 256\n",
        "train_iter = getTrainIter(7)\n",
        "weight_matrix = TEXT.vocab.vectors  # 构建权重矩阵\n",
        "weight_matrix.cuda()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "read data from <module 'posixpath' from '/usr/lib/python3.6/posixpath.py'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [-0.0054, -0.5103,  0.1257,  ...,  0.1709, -0.0147, -0.0972],\n",
              "        ...,\n",
              "        [ 0.5034, -0.4208, -0.3231,  ...,  0.3213,  0.2461,  0.5093],\n",
              "        [-0.4121, -1.2674,  1.0086,  ..., -0.6305, -0.9824, -0.0370],\n",
              "        [-0.6206, -0.3567, -0.8287,  ...,  0.5008,  0.8944, -0.2934]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJPI6gntZtJ5",
        "colab_type": "text"
      },
      "source": [
        "一堆生成one-hot表示的函数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqPRT6TW3stD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot(x, n_class, dtype=torch.float32):\n",
        "    # X shape: (batch), output shape: (batch, n_class)\n",
        "    x = x.long()\n",
        "    res = torch.zeros(x.shape[0], n_class, dtype=dtype, device=x.device)\n",
        "    res.scatter_(1, x.view(-1, 1), 1)\n",
        "    return res\n",
        "\n",
        "\n",
        "def to_onehotO(X, n_class):\n",
        "    # X shape: (batch, seq_len), output: seq_len elements of (batch, n_class)\n",
        "    return [one_hot(X[:, i], n_class) for i in range(X.shape[1])]\n",
        "\n",
        "\n",
        "def idx_to_onehot(i):\n",
        "    res = torch.zeros((len(TEXT.vocab), 1))\n",
        "    res[i] = 1\n",
        "    return res\n",
        "\n",
        "def char_to_onehot(c):\n",
        "    res = torch.zeros((len(TEXT.vocab), 1))\n",
        "    res[TEXT.vocab.stoi[c]] = 1\n",
        "    return res\n",
        "\n",
        "def sentence_to_onehot(idx):\n",
        "    res = torch.zeros((6*len(TEXT.vocab), batch_size), dtype=torch.long)\n",
        "    for i in range(batch_size):\n",
        "        for j in range(6):\n",
        "            res[idx[j][i]+j*len(TEXT.vocab)][i] = 1\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QODh2tBgZyyy",
        "colab_type": "text"
      },
      "source": [
        "全部的网络"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQSSAua3vqWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 输入: 整首诗(28*batch_size)输出: v1-v_ith_sentence-1(1*embedding dim)\n",
        "class CSM(nn.Module):\n",
        "    def __init__(self, vocab_size=len(TEXT.vocab), pad_idx=TEXT.vocab.stoi[TEXT.pad_token], embedding_dim=150, text_len=7, feature_size=200):\n",
        "        super().__init__()  # 调用nn.Module的构造函数进行初始化\n",
        "        # 使用embedding table构建语句到向量的映射\n",
        "        self.embedding = nn.Embedding(\n",
        "            vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.embedding.weight.data.copy_(weight_matrix)  # 载入由预训练词向量生成的权重矩阵\n",
        "        self.embedding.to(device)\n",
        "        self.relu = nn.ReLU()  # ReLU函数\n",
        "        self.bn = nn.BatchNorm1d(num_features=feature_size)\n",
        "        self.conv1 = nn.Conv1d(in_channels=embedding_dim,\n",
        "                               out_channels=feature_size, kernel_size=2)\n",
        "        self.conv2 = nn.Conv1d(in_channels=feature_size,\n",
        "                               out_channels=feature_size, kernel_size=2)\n",
        "        self.conv3 = nn.Conv1d(in_channels=feature_size,\n",
        "                               out_channels=feature_size, kernel_size=3)\n",
        "        self.conv4 = nn.Conv1d(in_channels=feature_size,\n",
        "                               out_channels=feature_size, kernel_size=3)\n",
        "\n",
        "    def forward(self, text, ith_sentence):  # 前向传播\n",
        "        ans = []\n",
        "        if training:\n",
        "            for j in range(1,ith_sentence): #生成v1-v_ith_sentence-1\n",
        "                embedded = self.embedding(text[(j-1)*7:j*7])\n",
        "                # print(text)\n",
        "                # batch_size*embedding_dim*text_len(7)\n",
        "                embedded = embedded.permute(1, 2, 0)\n",
        "                out = self.conv1(embedded)\n",
        "                out = self.bn(out)\n",
        "                out = self.relu(out)  # batch_size*feature_size*6\n",
        "                out = self.conv2(out)\n",
        "                out = self.bn(out)\n",
        "                out = self.relu(out)  # batch_size*feature_size*5\n",
        "                out = self.conv3(out)\n",
        "                out = self.bn(out)\n",
        "                out = self.relu(out)  # batch_size*feature_size*3\n",
        "                out = self.conv4(out)\n",
        "                out = self.relu(out)  # batch_size*feature_size*1\n",
        "                ans.append(out.squeeze())\n",
        "        return ans  # batch_size*feature_size*3 \n",
        "\n",
        "# 输入: vec_i 输出: u_i^j\n",
        "class RCMUnit(nn.Module):\n",
        "    def __init__(self, feature_size=200):\n",
        "        super().__init__()\n",
        "        self.relu = nn.ReLU()  # ReLU函数\n",
        "        self.U = nn.Linear(in_features=feature_size, out_features=feature_size)\n",
        "\n",
        "    def forward(self, vec):  # 前向传播\n",
        "        out = self.U(torch.transpose(vec, 0, 1))\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "# 输入: vec_1-vec_i, 输出: u_i^1-u_i^m组成的list\n",
        "class RCM(nn.Module):\n",
        "    def __init__(self, feature_size=200, num_of_unit=7):\n",
        "        super().__init__()\n",
        "        self.relu = nn.ReLU()  # ReLU函数\n",
        "        self.M = nn.Linear(in_features=2*feature_size,\n",
        "                           out_features=feature_size)\n",
        "        self.U = []\n",
        "        self.num_of_unit = num_of_unit\n",
        "        self.feature_size = feature_size\n",
        "        for i in range(0, num_of_unit-1):\n",
        "            self.U.append(RCMUnit().cuda())\n",
        "\n",
        "    def forward(self, vecs, ith_sentence):  # 前向传播\n",
        "        ans = []\n",
        "        h = torch.zeros((vecs[0].size()[0], self.feature_size)).cuda()\n",
        "        for i in range(0, ith_sentence-1):\n",
        "            out = torch.cat((vecs[i], h), dim=1)\n",
        "            out = self.M(out)\n",
        "            h = self.relu(out)\n",
        "        for j in range(0, self.num_of_unit-1):\n",
        "            out = self.U[j](torch.transpose(h, 0, 1))\n",
        "            ans.append(out)\n",
        "        return ans\n",
        "\n",
        "# 输入: u_i^j,w_j 输出: 最可能的第j+1个字\n",
        "class RGM(nn.Module):\n",
        "    def __init__(self, vocab_size=len(TEXT.vocab), feature_size=200, text_len=7):\n",
        "        super(RGM, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.R = nn.Linear(feature_size, feature_size)\n",
        "        self.H = nn.Linear(feature_size, feature_size)\n",
        "        self.X = nn.Linear(vocab_size, feature_size)\n",
        "        self.Y = nn.Linear(feature_size, vocab_size)\n",
        "        self.r = torch.zeros((feature_size, 1))\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, u, w, r):  # inputs: (batch, seq_len)\n",
        "        # 获取one-hot向量表示\n",
        "        e = idx_to_onehot(w).cuda()  # X是个list\n",
        "        ro = self.R(torch.transpose(r, 0, 1)) #feature_size*1\n",
        "        xo = self.X(torch.transpose(e, 0, 1)) #feature_size*1\n",
        "        ho = self.H(u) #feature_size*1\n",
        "        self.r = self.relu(ro+xo+ho)\n",
        "        y = self.Y(self.r)\n",
        "        self.r = torch.transpose(self.r, 0, 1)\n",
        "        return y, self.r #r为更新后状态 batch_size*feature_size*1, y为生成值 1*vocab_size*batch_size\n",
        "\n",
        "\n",
        "\n",
        "#输入: 28*batch_size(batch_size首诗), ith_sentence(要预测诗句的序号(2-4)), 输出: ans(所预测的诗句的词向量组成的tensor, batch_size*1050(1050为7*embedding_dim)) (需接一个decoder), lst(所预测的诗句(汉字)\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, vocab_size=len(TEXT.vocab), feature_size=200, text_len=7):\n",
        "        super(Model, self).__init__()\n",
        "        #创建用到的三个网络\n",
        "        self.csm = CSM()\n",
        "        self.rcm = RCM()\n",
        "        self.rgm = RGM()\n",
        "\n",
        "    def forward(self, text, ith_sentence):\n",
        "        vecs = self.csm(text, ith_sentence) #用CSM生成v_1-v_i vecs为一个list (ith_sentence-1)*\n",
        "        u = self.rcm(vecs, ith_sentence)  #\n",
        "        t = torch.zeros((200, 1), requires_grad=True).cuda()\n",
        "        length = u[-1].size()[0] #batch中诗的数目\n",
        "        #ans = torch.empty([], dtype=torch.long).cuda()\n",
        "        #print(ans.size())\n",
        "        #使用以下被注释的代码可以实现向量化生成整个batch的预测值\n",
        "        '''\n",
        "        for i in range(length):\n",
        "            out = torch.tensor([], requires_grad=True)\n",
        "            lst = []\n",
        "            lst.append(w)\n",
        "        lst = []\n",
        "        for j in range(6):\n",
        "            y, t = self.rgm(u[j].cuda(), w, t)\n",
        "            #print(y.size(),t.size())\n",
        "            #print(torch.argmax(y,dim=1).size())\n",
        "            w = TEXT.vocab.itos[torch.argmax(y, dim=1)]\n",
        "            #out = torch.cat((out, TEXT.vocab.vectors[torch.argmax(y, dim=1)]), dim=1)\n",
        "            out = torch.argmax(y,dim=1).unsqueeze(1)\n",
        "            #print(out.size())\n",
        "            #lst.append(w)\n",
        "            if j==0:\n",
        "                ans = out #out\n",
        "            else:\n",
        "                ans = torch.cat((ans, out), dim=1)\n",
        "            print(ans.size())\n",
        "        ans = torch.transpose(ans, 0, 1)\n",
        "        return ans, lst\n",
        "        '''\n",
        "        #在生成第二句时打印输出，从而人工评估训练效果\n",
        "        if(ith_sentence == 2):\n",
        "          for i in range(length):\n",
        "            out = TEXT.vocab.vectors[text[0][i]].unsqueeze(0) #将真实的首字的词向量添加到输出\n",
        "            w = text[7][i] #使用真实的首字对整句剩下的字进行预测\n",
        "            lst = []\n",
        "            lst.append(TEXT.vocab.itos[w])\n",
        "            for j in range(6):\n",
        "              y, t = self.rgm(u[j][i].cuda(), w, t)\n",
        "              w = torch.argmax(y, dim=1)\n",
        "              out = torch.cat((out, TEXT.vocab.vectors[torch.argmax(y, dim=1)]), dim=1) #横向叠加\n",
        "              #print(out.size())\n",
        "              lst.append(TEXT.vocab.itos[w])\n",
        "            #此时out 1*1050\n",
        "            if i==0:\n",
        "              ans = out\n",
        "            else:\n",
        "              ans = torch.cat((ans, out), dim=0) #纵向叠加\n",
        "          print(lst) #预测输出\n",
        "          true = [] #真实输出\n",
        "          for j in range(7):\n",
        "            true.append(TEXT.vocab.itos[text[j+7][length-1]]) #将真实的诗句逐字加入到列表\n",
        "          print(true)\n",
        "        else:\n",
        "          lst = []\n",
        "          for i in range(length):\n",
        "            out = TEXT.vocab.vectors[text[0][i]].unsqueeze(0)\n",
        "            #print(out.size())\n",
        "            w = text[7*(ith_sentence-1)][i] #使用真实的首字对整句剩下的字进行预测\n",
        "            for j in range(6): #循环预测\n",
        "              y, t = self.rgm(u[j][i].cuda(), w, t)\n",
        "              w = torch.argmax(y, dim=1)\n",
        "              out = torch.cat((out, TEXT.vocab.vectors[torch.argmax(y, dim=1)]), dim=1)\n",
        "            if i==0:\n",
        "              ans = out\n",
        "              print(out.size())\n",
        "            else:\n",
        "              ans = torch.cat((ans, out), dim=0)\n",
        "        return ans, lst"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cF9diuvOZ4pM",
        "colab_type": "text"
      },
      "source": [
        "训练所用函数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXPU6b7KZ6W4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#用于测试 生成输入诗句的下一句\n",
        "def put(str, ith_sentence):\n",
        "    s = tokenize(str)\n",
        "    l = []\n",
        "    print(s)\n",
        "    for w in s:\n",
        "        l.append([TEXT.vocab.stoi[w]])\n",
        "    lt = torch.tensor(l)\n",
        "    return model(lt.cuda(), ith_sentence)\n",
        "\n",
        "\n",
        "def fit(epoch):\n",
        "    start = time.time() #记录训练开始时间\n",
        "    for i in tqdm(range(1, epoch+1)):\n",
        "        for idx, batch in enumerate(train_iter):\n",
        "            for j in range(1,4): #生成2-4句\n",
        "                model.zero_grad()  # 将上次计算得到的梯度值清零\n",
        "                model.train()  # 将模型设为训练模式'\n",
        "                #print(batch.text[(j-1)*7:j*7].size())\n",
        "                predicted, wordlist = model(batch.text.cuda(), j+1)\n",
        "                comp = torch.zeros(0, requires_grad=True)\n",
        "                loss = 0\n",
        "                for i in range(j*7, (j+1)*7):\n",
        "                    comp = torch.cat((comp, TEXT.vocab.vectors[batch.text[i, :]]), dim=1)\n",
        "                loss = loss_function(predicted, comp)\n",
        "                loss.backward()  # 反向传播\n",
        "                optimizer.step()  # 修正模型\n",
        "            print(loss) #打印损失\n",
        "            #print(wordlist)\n",
        "    end = time.time() #记录训练结束时间\n",
        "    print('Time used: %ds' %(end-start)) #打印训练所用时间"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tnIZ0ipZelL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d3ec8912-775b-4e93-a17e-0f9d9e745bcd"
      },
      "source": [
        "model = Model()\n",
        "#loss_function = nn.functional.cross_entropy #使用交叉熵损失函数\n",
        "#loss_function = nn.functional.nll_loss\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.1)  # 使用Adam作为优化器\n",
        "model.cuda()\n",
        "loss_function = nn.functional.mse_loss  # 使用交叉熵损失函数\n",
        "#put(\"也 无 风 雨 也 无 晴\")\n",
        "#put(\"不要 搞个 大 新闻 呃 谔\")\n",
        "\n",
        "training = True\n",
        "fit(10)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['诗', '锱', '质', '享', '享', '更', '质']\n",
            "['诗', '就', '呻', '吟', '不', '要', '工']\n",
            "torch.Size([1, 1050])\n",
            "torch.Size([1, 1050])\n",
            "tensor(0.1104, grad_fn=<MeanBackward0>)\n",
            "['柳', '享', '质', '享', '享', '侵', '享']\n",
            "['柳', '陌', '菱', '塘', '一', '带', '疏']\n",
            "torch.Size([1, 1050])\n",
            "torch.Size([1, 1050])\n",
            "tensor(0.1115, grad_fn=<MeanBackward0>)\n",
            "['重', '质', '质', '侵', '享', '侵', '质']\n",
            "['重', '洋', '突', '涌', '大', '孱', '颜']\n",
            "torch.Size([1, 1050])\n",
            "torch.Size([1, 1050])\n",
            "tensor(0.1105, grad_fn=<MeanBackward0>)\n",
            "['小', '质', '质', '享', '享', '质', '享']\n",
            "['小', '对', '秋', '风', '梳', '白', '头']\n",
            "torch.Size([1, 1050])\n",
            "torch.Size([1, 1050])\n",
            "tensor(0.1099, grad_fn=<MeanBackward0>)\n",
            "['十', '锱', '质', '锱', '享', '更', '质']\n",
            "['十', '七', '年', '前', '鬓', '未', '秋']\n",
            "torch.Size([1, 1050])\n",
            "torch.Size([1, 1050])\n",
            "tensor(0.1129, grad_fn=<MeanBackward0>)\n",
            "['漫', '质', '质', '锱', '享', '篇', '质']\n",
            "['漫', '遣', '槎', '牙', '铁', '锁', '横']\n",
            "torch.Size([1, 1050])\n",
            "torch.Size([1, 1050])\n",
            "tensor(0.1106, grad_fn=<MeanBackward0>)\n",
            "['漾', '质', '质', '锱', '享', '更', '质']\n",
            "['漾', '漾', '清', '流', '照', '翠', '微']\n",
            "torch.Size([1, 1050])\n",
            "torch.Size([1, 1050])\n",
            "tensor(0.1122, grad_fn=<MeanBackward0>)\n",
            "['这', '享', '质', '享', '享', '讲', '享']\n",
            "['这', '僧', '不', '见', '凿', '头', '方']\n",
            "torch.Size([1, 1050])\n",
            "torch.Size([1, 1050])\n",
            "tensor(0.1116, grad_fn=<MeanBackward0>)\n",
            "['生', '缴', '质', '享', '享', '讲', '享']\n",
            "['生', '缚', '青', '宜', '不', '动', '尘']\n",
            "torch.Size([1, 1050])\n",
            "torch.Size([1, 1050])\n",
            "tensor(0.1123, grad_fn=<MeanBackward0>)\n",
            "['淮', '享', '质', '享', '享', '讲', '享']\n",
            "['淮', '水', '茫', '茫', '古', '渡', '头']\n",
            "torch.Size([1, 1050])\n",
            "torch.Size([1, 1050])\n",
            "tensor(0.1097, grad_fn=<MeanBackward0>)\n",
            "['论', '质', '质', '享', '享', '更', '质']\n",
            "['论', '功', '何', '似', '禹', '乘', '<R>']\n",
            "torch.Size([1, 1050])\n",
            "torch.Size([1, 1050])\n",
            "tensor(0.1120, grad_fn=<MeanBackward0>)\n",
            "['只', '质', '质', '享', '享', '更', '质']\n",
            "['只', '求', '学', '艺', '不', '知', '人']\n",
            "torch.Size([1, 1050])\n",
            "torch.Size([1, 1050])\n",
            "tensor(0.1103, grad_fn=<MeanBackward0>)\n",
            "['鬓', '讲', '番', '弘', '享', '更', '槁']\n",
            "['鬓', '影', '青', '青', '表', '帝', '庭']\n",
            "torch.Size([1, 1050])\n",
            "torch.Size([1, 1050])\n",
            "tensor(0.1143, grad_fn=<MeanBackward0>)\n",
            "['尖', '质', '质', '享', '享', '更', '享']\n",
            "['尖', '风', '料', '峭', '雨', '微', '冥']\n",
            "torch.Size([1, 1050])\n",
            "torch.Size([1, 1050])\n",
            "tensor(0.1090, grad_fn=<MeanBackward0>)\n",
            "['当', '质', '质', '享', '享', '讲', '舍']\n",
            "['当', '时', '高', '士', '见', '精', '微']\n",
            "torch.Size([1, 1050])\n",
            "torch.Size([1, 1050])\n",
            "tensor(0.1119, grad_fn=<MeanBackward0>)\n",
            "['有', '质', '番', '弘', '享', '更', '享']\n",
            "['有', '酒', '可', '以', '留', '人', '嬉']\n",
            "torch.Size([1, 1050])\n",
            "torch.Size([1, 1050])\n",
            "tensor(0.1120, grad_fn=<MeanBackward0>)\n",
            "['霞', '享', '质', '享', '享', '讲', '享']\n",
            "['霞', '彩', '云', '阴', '暮', '景', '中']\n",
            "torch.Size([1, 1050])\n",
            "torch.Size([1, 1050])\n",
            "tensor(0.1124, grad_fn=<MeanBackward0>)\n",
            "['金', '酲', '质', '弘', '享', '更', '享']\n",
            "['金', '身', '丈', '六', '曳', '袈', '裟']\n",
            "torch.Size([1, 1050])\n",
            "torch.Size([1, 1050])\n",
            "tensor(0.1137, grad_fn=<MeanBackward0>)\n",
            "['乐', '质', '质', '弘', '享', '薇', '弘']\n",
            "['乐', '事', '相', '寻', '满', '社', '春']\n",
            "torch.Size([1, 1050])\n",
            "torch.Size([1, 1050])\n",
            "tensor(0.1117, grad_fn=<MeanBackward0>)\n",
            "['无', '舍', '质', '弘', '享', '更', '质']\n",
            "['无', '数', '霜', '天', '归', '雁', '横']\n",
            "torch.Size([1, 1050])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-310e111005af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-78-736d31bac057>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 将模型设为训练模式'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0;31m#print(batch.text[(j-1)*7:j*7].size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m                 \u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m                 \u001b[0mcomp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-78-736d31bac057>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text, ith_sentence)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mith_sentence\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#使用真实的首字对整句剩下的字进行预测\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#循环预测\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m               \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrgm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m               \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m               \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-78-736d31bac057>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, u, w, r)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# 获取one-hot向量表示\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx_to_onehot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# X是个list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#feature_size*1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mxo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#feature_size*1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mho\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#feature_size*1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_state_dict_pre_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLQTD6GlWDiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#保存模型\n",
        "torch.save(model.state_dict(), '/content/drive/My Drive/Colab Notebooks/models/model.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}